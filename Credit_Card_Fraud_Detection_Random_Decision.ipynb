{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37252e7f",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection With Classification Algorithms In Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d7d71",
   "metadata": {},
   "source": [
    "Fraud transactions or fraudulent activities are significant issues in many industries like banking, insurance, etc. Especially for the banking industry, credit card fraud detection is a pressing issue to resolve.\n",
    "\n",
    "These industries suffer too much due to fraudulent activities towards revenue growth and lose customer’s trust. So these companies need to find fraud transactions before it becomes a big problem for them.  \n",
    "\n",
    "Unlike the other machine learning problems, in credit card fraud detection the target class distribution is not equally distributed. It is popularly known as the class imbalance problem or unbalanced data issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760fdba",
   "metadata": {},
   "source": [
    "This makes this problem even more challenging to solve.\n",
    "\n",
    "So In this article, we will explain to you how to build credit card fraud detection using different machine learning classification algorithms.\n",
    "\n",
    "Such as,\n",
    "\n",
    "1. Decision trees algorithm\n",
    "\n",
    "2. Random forest algorithm\n",
    "\n",
    "You will also get an idea about the impact of unbalanced data on the model’s performance.\n",
    "\n",
    "Let us give you the list of contents that we will discuss in the next few minutes. Just to give you a glimpse about the topics that you are going to learn from this article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58f1c4",
   "metadata": {},
   "source": [
    "Let’s begin the discussion by understanding why we need to find fraudulent transactions/activities in any industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd2c89",
   "metadata": {},
   "source": [
    "1. Why do we need to find fraud transactions?\n",
    "\n",
    "\n",
    "   Fraud Detection Approaches\n",
    "   \n",
    "    \n",
    "2. What is Credit Card Fraud Detection?\n",
    "\n",
    "\n",
    "3. Understanding of Credit Card Dataset \n",
    "\n",
    "\n",
    "\n",
    "   Data Explorations\n",
    "   \n",
    "   \n",
    "   \n",
    "4. Credit Card Data Preprocessing\n",
    "\n",
    "\n",
    "   Removing irrelevant columns/features\n",
    "\n",
    "\n",
    "\n",
    "   Checking null or nan values\n",
    "\n",
    "\n",
    "\n",
    "   Data Transformation\n",
    "\n",
    "\n",
    "\n",
    "   Splitting dataset \n",
    "\n",
    "\n",
    "\n",
    "5. Building Credit Card Fraud Detection using Machine Learning algorithms\n",
    "\n",
    "\n",
    "   Decision Tree Algorithm Overview\n",
    "\n",
    "\n",
    "   Random Forest Algorithm Overview\n",
    "\n",
    "\n",
    "6. Credit Card Fraud Detection with Decision Tree Algorithm\n",
    "\n",
    "\n",
    "   Decision tree algorithm Implementation using python sklearn library\n",
    "\n",
    "\n",
    "7. Credit Card Fraud Detection with Random Forest Algorithm\n",
    "\n",
    "\n",
    "   Random forest algorithm Implementation using sklearn library\n",
    "\n",
    "\n",
    "8. Why Accuracy not suitable for Data Imbalance Problems?\n",
    "\n",
    "\n",
    "9. Suitable evaluation metrics for imbalanced data\n",
    "\n",
    "\n",
    "   Decision Tree Classification model results\n",
    "\n",
    "\n",
    "   Random Forest Classification model results\n",
    "\n",
    "\n",
    "   AUC and ROC Curves\n",
    "\n",
    "\n",
    "10. Model Improvement Using Sampling Techniques\n",
    "\n",
    "\n",
    "   Applying Sampling Techniques \n",
    "\n",
    "\n",
    "   Decision tree classification after applying sampling techniques\n",
    "\n",
    "\n",
    "   Random Forest Tree Classifier after applying the sampling techniques\n",
    "\n",
    "\n",
    "11. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441b02b",
   "metadata": {},
   "source": [
    "# Why do we need to find fraud transactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d27cb",
   "metadata": {},
   "source": [
    "For many companies, fraud detection is a big problem because they find these fraudulent activities after they experience high loss. \n",
    "\n",
    "Fraud activities happen in all  industries. We can't say only particular companies/industries suffer from these fraudulent activities or transactions. \n",
    "\n",
    "But when it comes to financial-related companies, this fraud transaction becomes more of an issue/problem.  So these companies want to detect fraud transactions before the fraud activities turn into significant damage to their company.\n",
    "\n",
    "In the current generation, with high-end technology, still, on every 100 credit card transactions, 13% are falling into the fraudulent activities reported by the creditcards website.\n",
    "\n",
    "A survey paper mentioned that in the year 1997, 63% of companies experienced one fraud in the past two years, and in another year 1999, 57% of companies experienced at least one fraud in the last one year. \n",
    "\n",
    "Here the point is not only fraud activities increase, but the way of doing scams also increases badly. \n",
    "\n",
    "Companies suffer from detecting fraud, and due to these fraudulent activities, many companies worldwide have lost billions of dollars yearly.\n",
    "\n",
    "And one more thing, for any company, customer's trust is more important to achieve or reach some position in the business marketplace. If a company cannot find these fraudulent activities, companies lose customer's trust; then, they will suffer from customer churn.\n",
    "\n",
    "Fraud Detection Approaches\n",
    "So companies start to detect these fraud activities automatically by using smart technologies. \n",
    "\n",
    "First, companies hire few people only for the detection of these kinds of activities or transactions. But here they must and should be experts in this field or domain, and also the team should have knowledge of how frauds occur in particular domains. This requires more resources, such as people's effort and time.\n",
    "\n",
    "Second, companies changed manual processes to rule-based solutions. But this one also fails most of the time to detect frauds. \n",
    "\n",
    "Because in the real world, the way of doing frauds is changing drastically day by day. These rule-based systems follow some rules and conditions. If a new fraud process is different from others, then these systems fail. It requires adding that new rule to code and execute. \n",
    "\n",
    "Now companies are trying to adopt Artificial Intelligence or machine learning algorithms to detect frauds. Machine learning algorithms performed very well for this type of problem.  \n",
    "\n",
    "The payment gateway Stripe, for example — which can be integrated with the recurring payment provider Chargebee — uses an adaptive machine learning algorithm that evaluates risk in real-time and predicts whether a payment is likely to be fraudulent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529324b",
   "metadata": {},
   "source": [
    "# Fraud Detection Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e068c",
   "metadata": {},
   "source": [
    "So companies start to detect these fraud activities automatically by using smart technologies. \n",
    "\n",
    "First, companies hire few people only for the detection of these kinds of activities or transactions. But here they must and should be experts in this field or domain, and also the team should have knowledge of how frauds occur in particular domains. This requires more resources, such as people's effort and time.\n",
    "\n",
    "Second, companies changed manual processes to rule-based solutions. But this one also fails most of the time to detect frauds. \n",
    "\n",
    "Because in the real world, the way of doing frauds is changing drastically day by day. These rule-based systems follow some rules and conditions. If a new fraud process is different from others, then these systems fail. It requires adding that new rule to code and execute. \n",
    "\n",
    "Now companies are trying to adopt Artificial Intelligence or machine learning algorithms to detect frauds. Machine learning algorithms performed very well for this type of problem.  \n",
    "\n",
    "The payment gateway Stripe, for example — which can be integrated with the recurring payment provider Chargebee — uses an adaptive machine learning algorithm that evaluates risk in real-time and predicts whether a payment is likely to be fraudulent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3a369",
   "metadata": {},
   "source": [
    "# What is Credit Card Fraud Detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7000b",
   "metadata": {},
   "source": [
    "In the above section, we discussed the need for identifying fraudulent activities. The credit card fraud classification problem is used to find fraud transactions or fraudulent activities before they become a major problem to credit card companies. \n",
    "\n",
    "It uses the combination of fraud and non-fraud transactions from the historical data with different people's credit card transaction data to estimate fraud or non-fraud on credit card transactions.\n",
    "\n",
    "In this article, we are using the popular credit card dataset. Let’s understand the data before we start building the fraud detection models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090bbad",
   "metadata": {},
   "source": [
    "# Understanding of Credit Card Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6bc67",
   "metadata": {},
   "source": [
    "Before going to the model development part, we should have some knowledge about our dataset.\n",
    "Such as \n",
    "\n",
    "1.What is the size of the dataset?\n",
    "\n",
    "2.How many features does the dataset have?\n",
    "\n",
    "3.What are the target values?\n",
    "\n",
    "4.How many samples under each target value? , etc.\n",
    "\n",
    "If we know some information about the dataset, then we can decide what we have to do?. \n",
    "\n",
    "What are the questions we discussed above, all  we can explore by using the python pandas library. \n",
    "\n",
    "Let's jump to the data exploration part to find answers to all questions we have.\n",
    "\n",
    "Data Explorations\n",
    "First, we need to load the dataset. After downloading the dataset, extract the data and keep the file in the dataset under the project folder. \n",
    "\n",
    "We can quickly load it using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82014747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "fraud_df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6424689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22346660",
   "metadata": {},
   "source": [
    "Dataset has 284807 rows and 31 features. The result of the shape variable is a tuple that has the number of rows, number of columns of the dataset.\n",
    "\n",
    "We can see how the dataset looks like. The below command showcases  only five rows, head() by default, gives 5 samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd21d65",
   "metadata": {},
   "source": [
    "If you want to see more samples from the top, pass the number representing the number of samples you want to see like fraud_df.head(10). \n",
    "\n",
    "You can also see bottom samples by using the tail() function. Both are working in the same way.\n",
    "\n",
    "We can get all the list of feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882295e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns or Feature names :- \n",
      " Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Columns or Feature names :- \\n {fraud_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5570a",
   "metadata": {},
   "source": [
    "From this, we know Class is the target variable, and the remaining all are features of our dataset.\n",
    "\n",
    "Let's see what are the unique values we are having for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200c40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of target variable :- \n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique values of target variable :- \\n {fraud_df['Class'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8871e",
   "metadata": {},
   "source": [
    "The target variable Class has 0 and 1 values. Here\n",
    "\n",
    "1) 0 for non-fraudulent transactions\n",
    "\n",
    "2) 1 for fraudulent transactions\n",
    "\n",
    "Because we aim to find fraudulent transactions, the dataset's target value has a positive value for that. \n",
    "\n",
    "Still, What is pending in data exploration questions? \n",
    "\n",
    "yeah, we have to check how many samples each target class is having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca8a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples under each target value :- \n",
      " 0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples under each target value :- \\n {fraud_df['Class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0432b",
   "metadata": {},
   "source": [
    "Yeah, we have 284315 non-fraudulent transaction samples & 492 fraudulent transaction samples.\n",
    "\n",
    "We will discuss more about the data in the later sections of this article. \n",
    "\n",
    "You are going to know the variation of this number of samples and how much impact on the model's performance, how we can evaluate model performance for this data, etc.\n",
    "\n",
    "Still, now you only know about the dataset, such\n",
    "\n",
    "1. Dataset size\n",
    "\n",
    "2. Number of samples(rows) and features(columns)\n",
    "\n",
    "3. Names of the features\n",
    "\n",
    "4. About target variables, etc.\n",
    "\n",
    "Now we will discuss different data preprocessing techniques for our dataset. \n",
    "\n",
    "The data preprocessing techniques will be completely different from the text preprocessing techniques we discussed in the natural language processing data preprocessing techniques article "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb5ff1",
   "metadata": {},
   "source": [
    "# Credit Card Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78937d3",
   "metadata": {},
   "source": [
    "Preprocessing is the process of cleaning the dataset. In this step, we will apply different methods to clean the raw data to feed more meaningful data for the modeling phase. This method includes\n",
    "\n",
    "1. Remove duplicates or irrelevant samples\n",
    "\n",
    "2. Update missing values with the most relevant values \n",
    "\n",
    "3. Convert one data type to another example, categorical to integers, etc.\n",
    "\n",
    "Okay, now we will spend a couple of minutes checking the dataset and applying corresponding techniques to clean data. \n",
    "\n",
    "This step aims to improve the quality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fe19",
   "metadata": {},
   "source": [
    "# Removing irrelevant columns/features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985e809",
   "metadata": {},
   "source": [
    "In our dataset, only one irrelevant or not useful feature id Time. So we can drop that feature from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d787dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of feature names after removing Time column :- \n",
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# make sure which features are useful & which are not\n",
    "# we can remove irrelevant features\n",
    "fraud_df = fraud_df.drop(['Time'], axis=1)\n",
    "print(f\"list of feature names after removing Time column :- \\n{fraud_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b5a2d",
   "metadata": {},
   "source": [
    "If you want to drop more features from data, call drop() method with a list of feature names. \n",
    "\n",
    "We can observe no feature name Time in the list of feature names after dropping the Time feature/column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd66ff",
   "metadata": {},
   "source": [
    "# Checking null or nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b7cba",
   "metadata": {},
   "source": [
    "We can check the datatypes of all features and, at the same time, the number of non-null values of all features by using info() of pandas. \n",
    "\n",
    "Null or nan values are nothing, but there is no value for that particular feature or attribute.\n",
    "\n",
    "For example, these nan or null values are coming if the customer or user does not fill all information in the forms. Blank values are treated as null or nan values. \n",
    "\n",
    "It's okay; we can know all this information just by using info() from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d92134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   V1      284807 non-null  float64\n",
      " 1   V2      284807 non-null  float64\n",
      " 2   V3      284807 non-null  float64\n",
      " 3   V4      284807 non-null  float64\n",
      " 4   V5      284807 non-null  float64\n",
      " 5   V6      284807 non-null  float64\n",
      " 6   V7      284807 non-null  float64\n",
      " 7   V8      284807 non-null  float64\n",
      " 8   V9      284807 non-null  float64\n",
      " 9   V10     284807 non-null  float64\n",
      " 10  V11     284807 non-null  float64\n",
      " 11  V12     284807 non-null  float64\n",
      " 12  V13     284807 non-null  float64\n",
      " 13  V14     284807 non-null  float64\n",
      " 14  V15     284807 non-null  float64\n",
      " 15  V16     284807 non-null  float64\n",
      " 16  V17     284807 non-null  float64\n",
      " 17  V18     284807 non-null  float64\n",
      " 18  V19     284807 non-null  float64\n",
      " 19  V20     284807 non-null  float64\n",
      " 20  V21     284807 non-null  float64\n",
      " 21  V22     284807 non-null  float64\n",
      " 22  V23     284807 non-null  float64\n",
      " 23  V24     284807 non-null  float64\n",
      " 24  V25     284807 non-null  float64\n",
      " 25  V26     284807 non-null  float64\n",
      " 26  V27     284807 non-null  float64\n",
      " 27  V28     284807 non-null  float64\n",
      " 28  Amount  284807 non-null  float64\n",
      " 29  Class   284807 non-null  int64  \n",
      "dtypes: float64(29), int64(1)\n",
      "memory usage: 65.2 MB\n",
      "Dataset info :- \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset info :- \\n {fraud_df.info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb243978",
   "metadata": {},
   "source": [
    "See the result of dataset info(); \n",
    "\n",
    "it provides all information about our dataset, such as \n",
    "\n",
    "1.Total number of samples or rows\n",
    "\n",
    "2.Column names\n",
    "\n",
    "3.Number of non-null values\n",
    "\n",
    "4.The data type of each column\n",
    "\n",
    "Our dataset doesn’t have any null values because the total number features are 284807 that ranges from 0-284806; all features have the same number of samples/rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98d64a",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce0246",
   "metadata": {},
   "source": [
    "Except for the Amount column, all column’s values are within some range of values. So let's change the Amount columns values to a smaller range of numbers. \n",
    "\n",
    "We can simply do this process by using StandardScaler from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0554fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few values of Amount column :- \n",
      " 0    149.62\n",
      "1      2.69\n",
      "2    378.66\n",
      "3    123.50\n",
      "Name: Amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"few values of Amount column :- \\n {fraud_df['Amount'][0:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01912d3",
   "metadata": {},
   "source": [
    "See the values of the Amount feature values are in high range compared to other feature values. \n",
    "\n",
    "We will change values within a smaller range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "166f2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few values of Amount column after applying StandardScaler:- \n",
      " 0    0.244964\n",
      "1   -0.342475\n",
      "2    1.160686\n",
      "3    0.140534\n",
      "Name: norm_amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "fraud_df['norm_amount'] = StandardScaler().fit_transform(\n",
    "fraud_df['Amount'].values.reshape(-1,1))\n",
    "fraud_df = fraud_df.drop(['Amount'], axis=1)\n",
    "print(f\"few values of Amount column after applying StandardScaler:- \\n {fraud_df['norm_amount'][0:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbadc33",
   "metadata": {},
   "source": [
    "The scalar result is added as a new column with norm_amount name to the data frame after we drop the Amount column because there is no use with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece06a7",
   "metadata": {},
   "source": [
    "# Splitting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723eaec",
   "metadata": {},
   "source": [
    "Now we will take all independent columns (target column is dependent and the remaining all are independent columns to each other), as X and the target variable as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253745fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features and target creations\n",
    "X = fraud_df.drop(['Class'], axis=1)\n",
    "y = fraud_df[['Class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d4c3a",
   "metadata": {},
   "source": [
    "Now we need to split the whole dataset into train and test dataset. Training data is used at the time of building the model and a test dataset is used to evaluate trained models. \n",
    "\n",
    "By using the train_test_split method from the sklearn library we can do this process of splitting the dataset to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff60b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199364, 29)\n",
      "(85443, 29)\n",
      "(199364, 1)\n",
      "(85443, 1)\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset to train & test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf8617",
   "metadata": {},
   "source": [
    "Now our dataset is ready for building models. Let's jump to the development of  the model using machine learning algorithms such as decision tree and random forest classification algorithms from the sklearn module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d839d32",
   "metadata": {},
   "source": [
    "# Building Credit Card Fraud Detection using Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d02ee",
   "metadata": {},
   "source": [
    "Now we can build models using different machine learning algorithms. Before creating a model, we need to find the type of problem statement, which means is supervised or unsupervised algorithms. \n",
    "\n",
    "Our problem statement falls under the supervised learning problem means the dataset has a target value for each row or sample in the dataset. \n",
    "\n",
    "Supervised machine learning algorithms are two types \n",
    "\n",
    "1.Classification Algorithms\n",
    "\n",
    "2.Regression Algorithms\n",
    "\n",
    "Our problem statement belongs to what type of algorithms? \n",
    "\n",
    "Yeah, exactly.\n",
    "\n",
    "Credit card fraud detection is a classification problem. Target variable values of Classification problems have integer(0,1) or categorical values(fraud, non-fraud). The target variable of our dataset ‘Class’ has only two labels - 0 (non-fraudulent) and 1 (fraudulent).\n",
    "\n",
    "Before going further let us give an introduction for both decision tree classification and random forest classification. As in this article, we are going to use these two algorithms to build the credit card fraudulent activities identification model.\n",
    "\n",
    "1.Decision Tree Classification Algorithm\n",
    "\n",
    "2.Random Forest Classification Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63e70f",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0328b56",
   "metadata": {},
   "source": [
    "The decision tree is the simplest and most popular classification algorithm. For building the model the decision tree algorithm considers all the provided features of the data and comes up with the important features.\n",
    "\n",
    "Because of this advantage, the decision tree algorithms also used in identifying the importance of the feature metrics. Which used in handpicking the features. \n",
    "\n",
    "Once the important features identified then the model trains with the training data to come up with a set of rules. These rules used in predicting future cases or for the test dataset. \n",
    "\n",
    "This is a quick overview of the decision tree algorithm. If you want to learn more about the algorithm and implement in python, have a look at the below articles written by our team.\n",
    "\n",
    "1.How the decision tree learns from the training data\n",
    "\n",
    "2.Decision tree algorithm implementation in python\n",
    "\n",
    "3.Implementing decision tree in R\n",
    "\n",
    "4.How to visuvalizing the decsion tree\n",
    "\n",
    "Now let’s see a quick overview of the random forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de585b",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a45b80",
   "metadata": {},
   "source": [
    "The random forest algorithm falls under the ensemble learning algorithm category. In the random forest algorithm, we build N decision tree models.  \n",
    "\n",
    "All the models predict the target value. Using the majority voting approach the final target value will be predicted.\n",
    "\n",
    "For building the individual decision tree, the random forest algorithm randomly creates the sample dataset. These sample datasets are called as the bootstrap samples.\n",
    "\n",
    "Suppose we want to build the N decision trees to create the forest, the algorithm first creates N bootstrap samples. Later for each bootstrap sample, one decision tree model will build.\n",
    "\n",
    "This is a quick overview of the random forest algorithm, If you want to learn more, please have a look at the below articles.\n",
    "\n",
    "1. How bootstrap samples created in Ensembleme learning methods.\n",
    "\n",
    "2. End to end the working nature of the Random forest algorithm.\n",
    "\n",
    "3. Implementing the Random forest algorithm in python.\n",
    "\n",
    "Now let’s go to the implementation part, the crazy one 🙂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cba60",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection with Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee3984",
   "metadata": {},
   "source": [
    "We will use the DecisionTreeClassifier class from the sklearn library to train and evaluate models. We use X_train and y_train data for training purposes. X_train is a training dataset with features, and y_train is the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40c35f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training starts........\n",
      "Accuracy of model on test dataset :- 0.9995435553526912\n",
      "Confusion Matrix :- \n",
      " [[85292     4]\n",
      " [   35   112]]\n",
      "Classification Report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.97      0.76      0.85       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.98      0.88      0.93     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Model with randomforest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    "     # initialize object for DecisionTreeClassifier class\n",
    "     rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "     # train model by using fit method\n",
    "     print(\"Model training starts........\")\n",
    "     rf_classifier.fit(X_train, y_train.values.ravel())\n",
    "     acc_score = rf_classifier.score(X_test, y_test)\n",
    "     print(f'Accuracy of model on test dataset :- {acc_score}')\n",
    "     # predict result using test dataset\n",
    "     y_pred = rf_classifier.predict(X_test)\n",
    "     # confusion matrix\n",
    "     print(f\"Confusion Matrix :- \\n {confusion_matrix(y_test, y_pred)}\")\n",
    "     # classification report for f1-score\n",
    "     print(f\"Classification Report :- \\n {classification_report(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# calling random_forest_classifier\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f525b7b",
   "metadata": {},
   "source": [
    "Wow, our decision tree classification gives 99% accuracy on test data. \n",
    "\n",
    "But why f1-score on label 1 too less ?. \n",
    "\n",
    "Remember this point; we will discuss these metrics performances in the coming section of this article where we address the question\n",
    "\n",
    "Why the accuracy evaluation metric is not suitable for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c76b8f",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection with Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc9bf5",
   "metadata": {},
   "source": [
    "Same as the above decision tree implementation, we use X_train and y_train dataset for training purposes and X_test for evaluation. Here we train the ensemble technique model of RandomForestClassifier from the sklearn. We can see the variations in the evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27479cf1",
   "metadata": {},
   "source": [
    "# Random forest algorithm Implementation using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc5ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training starts........\n",
      "Accuracy of model on test dataset :- 0.9994967405170698\n",
      "Confusion Matrix :- \n",
      " [[85289     7]\n",
      " [   36   111]]\n",
      "Classification Report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.94      0.76      0.84       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.97      0.88      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Model with randomforest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    "     # initialize object for DecisionTreeClassifier class\n",
    "     rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "     # train model by using fit method\n",
    "     print(\"Model training starts........\")\n",
    "     rf_classifier.fit(X_train, y_train.values.ravel())\n",
    "     acc_score = rf_classifier.score(X_test, y_test)\n",
    "     print(f'Accuracy of model on test dataset :- {acc_score}')\n",
    "     # predict result using test dataset\n",
    "     y_pred = rf_classifier.predict(X_test)\n",
    "     # confusion matrix\n",
    "     print(f\"Confusion Matrix :- \\n {confusion_matrix(y_test, y_pred)}\")\n",
    "     # classification report for f1-score\n",
    "     print(f\"Classification Report :- \\n {classification_report(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "# calling random_forest_classifier\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f01ea8",
   "metadata": {},
   "source": [
    "Wow, this model's accuracy is also 99% great, but what about remaining evaluation metrics such as precision, recall, F1-score. \n",
    "\n",
    "Let's discuss these variations why it happens, all these in the coming section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d2d81",
   "metadata": {},
   "source": [
    "# Why Accuracy not suitable for Data Imbalance Problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb9739",
   "metadata": {},
   "source": [
    "What was the reason for not applying or not considering accuracy as a performance metric for this specific problem?\n",
    "\n",
    "Just take some time, think about it.\n",
    "\n",
    "Model training is completed; we got accuracy on the test set as 99%. \n",
    "\n",
    "But why this section? \n",
    "\n",
    "We are having various classification evaluation metrics to quantify the performance of the build model, accuracy is one method in that. What other methods we can apply?\n",
    "\n",
    "Now we will discuss our dataset and what are the best evaluation metrics for these kinds of problems.\n",
    "\n",
    "For this discussion, we have to remember two things that are previously discussed.\n",
    "\n",
    "1. The number of samples for each Class (target variable) value.\n",
    "\n",
    "2. Evaluation metrics at both the decision tree and random forest classification models.\n",
    "\n",
    "Do you remember the number of samples/rows for each target value? \n",
    "\n",
    "No? okay, let us check that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of samples under each target value :- \\n {fraud_df['Class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9ef95",
   "metadata": {},
   "source": [
    "See the number of samples for Class-1 (fraudulent) less than the samples for class-0 (non-fraudulent). \n",
    "\n",
    "This kind of dataset is called unbalanced data. Which means one class label samples are  higher and dominating the other class label. \n",
    "\n",
    "For a balanced dataset, accuracy is suitable because we take the divided value of the correctly predicted samples count with the total number of samples for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc5638",
   "metadata": {},
   "source": [
    "# Accuracy = number of correctly predicted samples / total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bdb19",
   "metadata": {},
   "source": [
    "For example. \n",
    "\n",
    "If our dataset has 20 samples, out of that 2 for Class 0 & 18 for Class 1. Our trained model correctly predicted 17 samples out of 18 Class-1 samples and 0 samples out of 2 Class-0 samples. \n",
    "\n",
    "What is the accuracy value for this? 85%.\n",
    "\n",
    "But this is not correct, right? Because the model doesn’t even predict one sample correctly for Class-0 samples, but we got 85% accuracy. \n",
    "\n",
    "For an unbalanced dataset, a list of evaluation metrics are available. In the next section, we will discuss this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb6de2",
   "metadata": {},
   "source": [
    "# Suitable evaluation metrics for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281eb7a",
   "metadata": {},
   "source": [
    "So which all metrics are suitable for unbalanced data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca9a95",
   "metadata": {},
   "source": [
    "We can use any of the below-mentioned metrics for unbalanced or skewed datasets.\n",
    "\n",
    "1.Recall\n",
    "\n",
    "2.Precision\n",
    "\n",
    "3.F1-score\n",
    "\n",
    "4.Area Under ROC curve.\n",
    "\n",
    "We can see the huge difference among different evaluation metrics for both classifications (decision tree & random forest) models. \n",
    "\n",
    "Do you remember we mentioned at model development stage, accuracy, classification report, etc. ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606a2b7",
   "metadata": {},
   "source": [
    "Here we have to discuss a few terms and formulae related to confusion matrix, precision, recall & F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70069d12",
   "metadata": {},
   "outputs": [],
   "source": [
    " #                  PREDICTED   PREDICTED\n",
    " #                  Positive    Negative\n",
    "              \n",
    " #ACTUAL Positive   TP          FN\n",
    " #ACTUAL Negative   FP          TN  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631150f0",
   "metadata": {},
   "source": [
    "1. True Positive (TP):-  \n",
    "The number of positive labels correctly predicted by trained models.  This means the number of Class-1 samples correctly predicted as Class-1.\n",
    "\n",
    "\n",
    "2. True Negative (TN):-\n",
    "The number of negative labels correctly predicted by trained models.  This means the number of Class-0 samples correctly predicted as Class-0.\n",
    "\n",
    "\n",
    "3. False Positive (FP):-  \n",
    "The number of positive labels incorrectly predicted by trained models. This means the number of Class-1 samples incorrectly predicted as Class-0.\n",
    "\n",
    "\n",
    "4. False Negative (FN):-  \n",
    "The number of negative labels incorrectly predicted by trained models.  This means the number of Class-0 samples incorrectly predicted as Class-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508c516",
   "metadata": {},
   "source": [
    "Formulae\n",
    "\n",
    "1.Recall = TP / (TP + FN)\n",
    "\n",
    "2.Precision = TP / (TP + FP)\n",
    "\n",
    "3.F1-Score = 2*P*R / (P + R) here P for Precision, R for Recall\n",
    "\n",
    "Both classification models got accuracy scores as 99%. \n",
    "\n",
    "But when we observe the result of the classification report of both classifiers, f1-score for Class-0 got 100%, but for Class-1, F1-scores are significantly less. \n",
    "\n",
    "All these variations occur due to the unbalanced or skewed dataset. \n",
    "\n",
    "Why f1-score for class-0 100%? \n",
    "\n",
    "Because of the number of samples for class-0 (2 lakhs). The number of samples for Class-0 is very high than the Class-1 samples.\n",
    "\n",
    "So what we need to do here is handle an unbalanced dataset. If you want to learn more about it, check the Best ways to handle unbalanced data in the machine learning article which explained various ways to handle the imbalanced data.\n",
    "\n",
    "One more thing is left for discussion in this section, which is about areas under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3038ba6",
   "metadata": {},
   "source": [
    "# AUC and ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b63fc4",
   "metadata": {},
   "source": [
    "Area Under ROC curve is another evaluation metric for classification problems. This is mostly suitable for skewed datasets. It tells us about model performance, such as the model's capability to distinguish between target classes. \n",
    "\n",
    "The effective model has a higher Area Under the ROC curve value. Here we measure the ability of class separability of a model by using the Area Under ROC curve.\n",
    "\n",
    "Good models have AUC value near to 1, and the worst models have AUC value near 0.\n",
    "\n",
    "All the model performance methods help in the measuring the performance of the model based on the problem, but how to build the best models when we face with the data imbalance issue?\n",
    "\n",
    "For that, we need to apply different sampling methods to the data before building the models.\n",
    "\n",
    "Let’s see how sampling methods improve model performance, and how much AUC score for that model in the coming section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca52d8",
   "metadata": {},
   "source": [
    "# Model Improvement Using Sampling Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adda39a",
   "metadata": {},
   "source": [
    "Data sampling is the statistical method for selecting data points (here, the data point is a single row) from the whole dataset. In machine learning problems, there are many sampling techniques available.\n",
    "\n",
    "Here we take undersampling and oversampling strategies for handling imbalanced data.  \n",
    "\n",
    "What is this undersampling and oversampling?\n",
    "Let us take an example of a dataset that has nine samples. \n",
    "\n",
    "1. Six samples belong to class-0,\n",
    "\n",
    "2. Three samples belong to class-1\n",
    "\n",
    "Oversampling = 6 class-0 samples x  2 times of class-1 samples of 3\n",
    "\n",
    "Undersampling = 3 Class-1 samples x 3 samples from Class-0\n",
    "\n",
    "Here what we are trying to do is the number of samples of both target classes to be equal. \n",
    "\n",
    "In the oversampling technique, samples are repeated, and the dataset size is larger than the original dataset.\n",
    "\n",
    "In the undersampling technique, samples are not repeated, and the dataset size is less than the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819029d",
   "metadata": {},
   "source": [
    "# Applying Sampling Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f126a9",
   "metadata": {},
   "source": [
    "For undersampling techniques, we are checking the number of samples of both classes and selecting the smaller number and taking random samples from other class samples to create a new dataset.  \n",
    "\n",
    "The new dataset has an equal number of samples for both target classes.\n",
    "\n",
    "This is a whole process of undersampling, and now we are going to implement this entire process using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1aa66b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for each class :- \n",
      " 0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n",
      "Non Fraudulent Numbers :- 284315\n",
      "Fraudulent Numbers :- 492\n"
     ]
    }
   ],
   "source": [
    "## Target class distribution\n",
    "class_val = fraud_df['Class'].value_counts()\n",
    "print(f\"Number of samples for each class :- \\n {class_val}\")\n",
    "non_fraud = class_val[0]\n",
    "fraud = class_val[1]\n",
    "print(f\"Non Fraudulent Numbers :- {non_fraud}\")\n",
    "print(f\"Fraudulent Numbers :- {fraud}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2812111",
   "metadata": {},
   "source": [
    "The above is the target class distributions, now let's see how we can change this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ddd7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Equal both the target samples to the same level\n",
    "# take indexes of non fraudulent\n",
    "nonfraud_indexies = fraud_df[fraud_df.Class == 0].index\n",
    "fraud_indices = np.array(fraud_df[fraud_df['Class'] == 1].index)\n",
    "# take random samples from non fraudulent that are equal to fraudulent samples\n",
    "random_normal_indexies = np.random.choice(nonfraud_indexies, fraud, replace=False)\n",
    "random_normal_indexies = np.array(random_normal_indexies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4a009",
   "metadata": {},
   "source": [
    "Here first, we take indexes of both classes and randomly choose Class-0 samples indexes that are equal to the number of Class-1 samples. \n",
    "\n",
    "In the below code snippet, Combine both classes indexes. Then we extract all features of gathered indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3738da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Equal both the target samples to the same level\n",
    "# take indexes of non fraudulent\n",
    "nonfraud_indexies = fraud_df[fraud_df.Class == 0].index\n",
    "fraud_indices = np.array(fraud_df[fraud_df['Class'] == 1].index)\n",
    "# take random samples from non fraudulent that are equal to fraudulent samples\n",
    "random_normal_indexies = np.random.choice(nonfraud_indexies, fraud, replace=False)\n",
    "random_normal_indexies = np.array(random_normal_indexies)\n",
    "\n",
    "\n",
    "## Undersampling techniques\n",
    "\n",
    "# concatenate both indices of fraud and non fraud\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indexies])\n",
    "\n",
    "#extract all features from whole data for under sample indices only\n",
    "under_sample_data = fraud_df.iloc[under_sample_indices, :]\n",
    "\n",
    "# now we have to divide under sampling data to all features & target\n",
    "x_undersample_data = under_sample_data.drop(['Class'], axis=1)\n",
    "y_undersample_data = under_sample_data[['Class']]\n",
    "# now split dataset to train and test datasets as before\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(\n",
    "x_undersample_data, y_undersample_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993ff0c",
   "metadata": {},
   "source": [
    "The above code first divides features and targets as x_undersample_data and y_undersample_data and then splits new undersample data into train and test dataset.\n",
    "\n",
    "Okay, now we will call both classifiers with these new under sampling train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1fa406",
   "metadata": {},
   "source": [
    "# Decision tree classification after applying sampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4516e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training start........\n",
      "Model training completed\n",
      "Accuracy of model on test dataset :- 0.9187817258883249\n",
      "Confusion Matrix :- \n",
      " [[97  9]\n",
      " [ 7 84]]\n",
      "Classification Report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       106\n",
      "           1       0.90      0.92      0.91        91\n",
      "\n",
      "    accuracy                           0.92       197\n",
      "   macro avg       0.92      0.92      0.92       197\n",
      "weighted avg       0.92      0.92      0.92       197\n",
      "\n",
      "AROC score :- \n",
      " 0.9190856313497823\n"
     ]
    }
   ],
   "source": [
    "## DecisionTreeClassifier after applying undersampling technique\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def decision_tree_classification(X_train, y_train, X_test, y_test):\n",
    " # initialize object for DecisionTreeClassifier class\n",
    " dt_classifier = DecisionTreeClassifier()\n",
    " # train model by using fit method\n",
    " print(\"Model training start........\")\n",
    " dt_classifier.fit(X_train, y_train.values.ravel())\n",
    " print(\"Model training completed\")\n",
    " acc_score = dt_classifier.score(X_test, y_test)\n",
    " print(f'Accuracy of model on test dataset :- {acc_score}')\n",
    " # predict result using test dataset\n",
    " y_pred = dt_classifier.predict(X_test)\n",
    " # confusion matrix\n",
    " print(f\"Confusion Matrix :- \\n {confusion_matrix(y_test, y_pred)}\")\n",
    " # classification report for f1-score\n",
    " print(f\"Classification Report :- \\n {classification_report(y_test, y_pred)}\")\n",
    " print(f\"AROC score :- \\n {roc_auc_score(y_test, y_pred)}\")\n",
    "\n",
    "# calling decision tree classifier function \n",
    "decision_tree_classification(X_train_sample, y_train_sample, \n",
    "X_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191523d",
   "metadata": {},
   "source": [
    "# Random Forest Tree Classifier after applying the sampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7029ecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training start........\n",
      "Accuracy of model on test dataset :- 0.9644670050761421\n",
      "Confusion Matrix :- \n",
      " [[104   2]\n",
      " [  5  86]]\n",
      "Classification Report :- \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       106\n",
      "           1       0.98      0.95      0.96        91\n",
      "\n",
      "    accuracy                           0.96       197\n",
      "   macro avg       0.97      0.96      0.96       197\n",
      "weighted avg       0.96      0.96      0.96       197\n",
      "\n",
      "AROC score :- \n",
      " 0.9630935102633216\n"
     ]
    }
   ],
   "source": [
    "## RandomForestClassifier after apply the undersampling techniques\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def random_forest_classifier(X_train, y_train, X_test, y_test):\n",
    " # initialize object for DecisionTreeClassifier class\n",
    " rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    " # train model by using fit method\n",
    " print(\"Model training start........\")\n",
    " rf_classifier.fit(X_train, y_train.values.ravel())\n",
    " acc_score = rf_classifier.score(X_test, y_test)\n",
    " print(f'Accuracy of model on test dataset :- {acc_score}')\n",
    " # predict result using test dataset\n",
    " y_pred = rf_classifier.predict(X_test)\n",
    " # confusion matrix\n",
    " print(f\"Confusion Matrix :- \\n {confusion_matrix(y_test, y_pred)}\")\n",
    " # classification report for f1-score\n",
    " print(f\"Classification Report :- \\n {classification_report(y_test, y_pred)}\")\n",
    " # area under roc curve\n",
    " print(f\"AROC score :- \\n {roc_auc_score(y_test, y_pred)}\")\n",
    "\n",
    "random_forest_classifier(X_train_sample, y_train_sample, X_test_sample, y_test_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaecb4",
   "metadata": {},
   "source": [
    "See, the results of the F1-score for both target values are 95%, and the Area Under ROC curve is near to 1. \n",
    "\n",
    "For the best models, we have the AUROC value near to 1. Here we implemented the undersampling technique; you can apply oversampling also like an undersampling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129990e1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d20597",
   "metadata": {},
   "source": [
    "Finally, our model gives 94% of the Area Under the ROC curve value. We can improve model results by adding more trees or applying additional data preprocessing techniques, etc. \n",
    "\n",
    "Not only decision trees or random forest classifiers suitable for this problem. You can try with other machine learning classification algorithms such as Support Vector Machines (SVM), k-nearest neighbors, etc.  to check how different algorithms are performed on classifying fraudulent activities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
